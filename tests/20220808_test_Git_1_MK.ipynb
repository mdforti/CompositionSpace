{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3d54e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import os\n",
    "import json \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import h5py\n",
    "from jupyterlab_h5web import H5Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dc0f43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from compositionspace.datautils import DataPreparation\n",
    "from compositionspace.segmentation import CompositionClustering\n",
    "from compositionspace.postprocessing import DataPostprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1d069f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run = False\n",
    "if run is True:\n",
    "    data = DataPreparation(\"experiment_params.yaml\")\n",
    "    df_lst, files, ions, rrngs= data.get_apt_dataframe()\n",
    "    # print(f\"{df_lst}, type {type(df_lst)}\")\n",
    "    # print(files)\n",
    "    # print(ions)\n",
    "    # print(rrngs)\n",
    "\n",
    "    for idx, file in enumerate(files):\n",
    "        org_file = df_lst[idx]\n",
    "        atoms_spec = []\n",
    "        c = np.unique(rrngs.comp.values)\n",
    "        for i in range(len(c)):\n",
    "            range_element = rrngs[rrngs['comp']=='{}'.format(c[i])]\n",
    "            total, count = data.atom_filter(org_file, range_element)\n",
    "            name = i\n",
    "            total[\"spec\"] = [name for j in range(len(total))]\n",
    "            atoms_spec.append(total)\n",
    "    # print(atoms_spec)\n",
    "    df_atom_spec = pd.concat(atoms_spec)\n",
    "    # print(df_atom_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22629265-7afc-48e7-93f4-dce993626ecd",
   "metadata": {},
   "source": [
    "## Load positions and iontypes from paraprobe-transcoder and paraprobe-ranger results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecd3a7e2-6c3a-470c-8c64-e7fdad7e44fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_fnm = \"data/PARAPROBE.Transcoder.Results.SimID.636502001.nxs\"\n",
    "range_fnm = \"data/PARAPROBE.Ranger.Results.SimID.636502001.nxs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d615c926-17f4-4ba2-93d9-acfe0230cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H5Web(recon_fnm)\n",
    "# H5Web(range_fnm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4cfaab3-72ab-4dfa-92d4-6d6349a533fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (4868202, 3), type <class 'numpy.ndarray'>, dtype float32\n",
      "73, iontypes {'ion0': ('unknown iontype', 0), 'ion1': ('C ++', 1), 'ion2': ('C +', 2), 'ion3': ('O +', 3), 'ion4': ('Ti ++', 4), 'ion5': ('Ti ++', 5), 'ion6': ('Ti ++', 6), 'ion7': ('Ti ++', 7), 'ion8': ('Fe ++', 8), 'ion9': ('Fe +', 9), 'ion10': ('Fe ++', 10), 'ion11': ('Fe ++', 11), 'ion12': ('Fe ++', 12), 'ion13': ('Fe +', 13), 'ion14': ('Al ++', 14), 'ion15': ('Al +++', 15), 'ion16': ('Si ++', 16), 'ion17': ('Si ++', 17), 'ion18': ('Si ++', 18), 'ion19': ('Ti O ++', 19), 'ion20': ('Ti O ++', 20), 'ion21': ('Ti O ++', 21), 'ion22': ('Ti O ++', 22), 'ion23': ('Ti O ++', 23), 'ion24': ('Ti O +', 24), 'ion25': ('Ti O +', 25), 'ion26': ('Ti O +', 26), 'ion27': ('Ti O +', 27), 'ion28': ('Cr ++', 28), 'ion29': ('Cr ++', 29), 'ion30': ('Cr ++', 30), 'ion31': ('Cr +', 31), 'ion32': ('Cr +', 32), 'ion33': ('Mn ++', 33), 'ion34': ('Mn +', 34), 'ion35': ('Co ++', 35), 'ion36': ('Y +++', 36), 'ion37': ('Y ++', 37), 'ion38': ('Ga ++', 38), 'ion39': ('Ga +', 39), 'ion40': ('Cr O ++', 40), 'ion41': ('Cr O ++', 41), 'ion42': ('Cr O +', 42), 'ion43': ('Y O ++', 43), 'ion44': ('Fe H +', 44), 'ion45': ('Ni +', 45), 'ion46': ('Ni +', 46), 'ion47': ('Ni ++', 47), 'ion48': ('Ni +', 48), 'ion49': ('Ni ++', 49), 'ion50': ('V ++', 50), 'ion51': ('Mo ++', 51), 'ion52': ('Mo ++', 52), 'ion53': ('Mo ++', 53), 'ion54': ('Mo ++', 54), 'ion55': ('Mo ++', 55), 'ion56': ('Mo ++', 56), 'ion57': ('Mo', 57), 'ion58': ('B +', 58), 'ion59': ('B ++', 59), 'ion60': ('O H +', 60), 'ion61': ('Si O +', 61), 'ion62': ('Fe O +', 62), 'ion63': ('Fe O +', 63), 'ion64': ('Fe O +', 64), 'ion65': ('Ti O O +', 65), 'ion66': ('Al O ++', 66), 'ion67': ('Al O +', 67), 'ion68': ('Ti C +++', 68), 'ion69': ('Ti C +', 69), 'ion70': ('Ti C +', 70), 'ion71': ('O H H +', 71), 'ion72': ('As ++', 72)}\n",
      "shape (4868202, 1), type <class 'numpy.ndarray'>, dtype uint8\n"
     ]
    }
   ],
   "source": [
    "h5r = h5py.File(recon_fnm, \"r\")\n",
    "trg = \"/entry1/atom_probe/reconstruction/reconstructed_positions\"\n",
    "xyz = h5r[trg][:, :]\n",
    "print(f\"shape {np.shape(xyz)}, type {type(xyz)}, dtype {xyz.dtype}\")\n",
    "\n",
    "trg = \"/entry1/atom_probe/ranging/peak_identification\"\n",
    "n_ion_types = len(h5r[trg])\n",
    "iontypes = {}\n",
    "for ion_id in np.arange(0, n_ion_types):\n",
    "    iontypes[f\"ion{ion_id}\"] = (str(h5r[f\"{trg}/ion{ion_id}/name\"][:].astype(str)[0]), np.uint8(ion_id))\n",
    "print(f\"{n_ion_types}, iontypes {iontypes}\")\n",
    "h5r.close()\n",
    "\n",
    "h5r = h5py.File(range_fnm, \"r\")\n",
    "trg = \"/entry1/process1/apply_existent_ranging\"\n",
    "ityp = h5r[f\"{trg}/iontypes\"][:]\n",
    "print(f\"shape {np.shape(ityp)}, type {type(ityp)}, dtype {ityp.dtype}\")\n",
    "h5r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e06e08a0-2049-440e-ac2e-c2c70f5277fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ceil_to_multiple(number, multiple):\n",
    "    return multiple * np.ceil(number / multiple)\n",
    "\n",
    "def floor_to_multiple(number, multiple):\n",
    "    return multiple * np.floor(number / multiple)\n",
    "\n",
    "# ceil to a multiple of 1.5\n",
    "# print(ceil_to_multiple(23.0000000000000000000000000000000000000, 1.5))\n",
    "# floor to a multiple of 1.5\n",
    "# print(floor_to_multiple(-23.0000000000000000000000000000000000000, 1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df8aa92-a3eb-4f43-b150-549de68af9e4",
   "metadata": {},
   "source": [
    "## Generate NXapm_composition_space results file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "380fefb3-0d0e-4d20-88d5-7a97c88d46c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_name = \"apm.composition.space.nxs\"\n",
    "h5w = h5py.File(output_file_name, \"w\")\n",
    "trg = \"/entry1\"\n",
    "grp = h5w.create_group(\"/entry1\")\n",
    "NX_APPDEF_VERSION = \"nexus-fairmat-proposal successor of 9636feecb79bb32b828b1a9804269573256d7696\"\n",
    "NX_APPDEF_NAME = \"NXapm_composition_space\"\n",
    "grp.attrs[\"version\"] = NX_APPDEF_VERSION\n",
    "grp.attrs[\"NX_class\"] = \"NXentry\"\n",
    "dst = h5w.create_dataset(f\"{trg}/definition\", data=NX_APPDEF_NAME)\n",
    "h5w.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df2fbfa-4c55-4cc0-b2c2-31af79e72a8b",
   "metadata": {},
   "source": [
    "## Voxelize with rectangular transfer function without creating slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33255c0b-04a8-4b84-9d70-c2ce3a62cfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (4868202,)\n",
      "aabb3d [-32.  32.], extent 32\n",
      "[-30. -28. -26. -24. -22. -20. -18. -16. -14. -12. -10.  -8.  -6.  -4.\n",
      "  -2.   0.   2.   4.   6.   8.  10.  12.  14.  16.  18.  20.  22.  24.\n",
      "  26.  28.  30.  32.]\n",
      "aabb3d [-32.  32.], extent 32\n",
      "[-30. -28. -26. -24. -22. -20. -18. -16. -14. -12. -10.  -8.  -6.  -4.\n",
      "  -2.   0.   2.   4.   6.   8.  10.  12.  14.  16.  18.  20.  22.  24.\n",
      "  26.  28.  30.  32.]\n",
      "aabb3d [ 0. 72.], extent 36\n",
      "[ 2.  4.  6.  8. 10. 12. 14. 16. 18. 20. 22. 24. 26. 28. 30. 32. 34. 36.\n",
      " 38. 40. 42. 44. 46. 48. 50. 52. 54. 56. 58. 60. 62. 64. 66. 68. 70. 72.]\n",
      "[237 421 725 456 555 398 327 616 788 731]\n",
      "2127\n"
     ]
    }
   ],
   "source": [
    "# print(type(df_lst))\n",
    "column_names = ['x', 'y', 'z']\n",
    "# initialize extent (number of cells) along x, y, z axes\n",
    "extent = [0, 0, 0]\n",
    "# initialize min, max bounds for x, y, z\n",
    "aabb3d = np.reshape([np.finfo(np.float32).max, np.finfo(np.float32).min,\n",
    "          np.finfo(np.float32).max, np.finfo(np.float32).min,\n",
    "          np.finfo(np.float32).max, np.finfo(np.float32).min], (3, 2), order=\"C\")\n",
    "# print(aabb3d)\n",
    "n_ions = np.shape(xyz)[0]\n",
    "voxel_identifier = np.asarray(np.zeros(n_ions), np.uint32)\n",
    "print(f\"shape {np.shape(voxel_identifier)}\")\n",
    "# edge length of cubic cells/voxels in nm\n",
    "dedge = 2.0\n",
    "for axis_id in [0, 1, 2]:\n",
    "    column_name = column_names[axis_id]\n",
    "    # i = np.asarray(df_lst[0].loc[:, column_name], np.float32)\n",
    "    aabb3d[axis_id, 0] = floor_to_multiple(np.min((aabb3d[axis_id, 0], np.min(xyz[:, axis_id]))), dedge)\n",
    "    aabb3d[axis_id, 1] = ceil_to_multiple(np.max((aabb3d[axis_id, 1], np.max(xyz[:, axis_id]))), dedge)\n",
    "    extent[axis_id] = np.uint32((aabb3d[axis_id, 1] - aabb3d[axis_id, 0]) / dedge)\n",
    "    print(f\"aabb3d {aabb3d[axis_id, :]}, extent {extent[axis_id]}\")\n",
    "    bins = np.linspace(aabb3d[axis_id, 0] + dedge, aabb3d[axis_id, 0] + (extent[axis_id] * dedge), num=extent[axis_id], endpoint=True)\n",
    "    print(bins)\n",
    "    if axis_id == 0:\n",
    "        voxel_identifier = voxel_identifier + (np.asarray(np.digitize(xyz[:, axis_id], bins, right=True), np.uint32) * 1)\n",
    "    elif axis_id == 1:\n",
    "        voxel_identifier = voxel_identifier + (np.asarray(np.digitize(xyz[:, axis_id], bins, right=True), np.uint32) * np.uint32(extent[0]))\n",
    "    else:\n",
    "        voxel_identifier = voxel_identifier + (np.asarray(np.digitize(xyz[:, axis_id], bins, right=True), np.uint32) * np.prod(np.uint32(extent[0:1])))\n",
    "print(voxel_identifier[0:10])\n",
    "print(np.max(voxel_identifier))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e839ac6-c719-4504-9400-80e51d278c04",
   "metadata": {},
   "source": [
    "Report results of voxelization and metadata for the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9efae095-f859-4ae9-9a64-fa11f08979db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voxelization\n",
    "h5w = h5py.File(output_file_name, \"a\")\n",
    "trg = \"/entry1/voxelization\"\n",
    "grp = h5w.create_group(f\"{trg}\")\n",
    "grp.attrs[\"NX_class\"] = \"NXprocess\"\n",
    "dst = h5w.create_dataset(f\"{trg}/sequence_index\", data=np.uint64(1))\n",
    "trg = \"/entry1/voxelization/cg_grid\"\n",
    "grp = h5w.create_group(f\"{trg}\")\n",
    "grp.attrs[\"NX_class\"] = \"NXcg_grid\"\n",
    "dst = h5w.create_dataset(f\"{trg}/dimensionality\", data=np.uint64(3))\n",
    "c = np.prod(extent)\n",
    "dst = h5w.create_dataset(f\"{trg}/cardinality\", data=np.uint64(c))\n",
    "dst = h5w.create_dataset(f\"{trg}/origin\", data=np.asarray([aabb3d[0, 0], aabb3d[1, 0], aabb3d[2, 0]], np.float64))\n",
    "dst = h5w.create_dataset(f\"{trg}/symmetry\", data=\"cubic\")\n",
    "dst = h5w.create_dataset(f\"{trg}/cell_dimensions\", data=np.asarray([dedge, dedge, dedge], np.float64))\n",
    "dst.attrs[\"unit\"] = \"nm\"\n",
    "dst = h5w.create_dataset(f\"{trg}/extent\", data=np.asarray(extent, np.uint32))  # max. 2*32 cells\n",
    "identifier_offset = 0\n",
    "dst = h5w.create_dataset(f\"{trg}/identifier_offset\", data=np.uint64(identifier_offset))  # start counting cells from 0\n",
    "\n",
    "voxel_id = identifier_offset\n",
    "position = np.zeros([c, 3], np.float64)\n",
    "for k in np.arange(0, extent[2]):\n",
    "    z = aabb3d[2, 0] + (0.5 + k) * dedge\n",
    "    for j in np.arange(0, extent[1]):\n",
    "        y = aabb3d[1, 0] + (0.5 + j) * dedge\n",
    "        for i in np.arange(0, extent[0]):\n",
    "            x = aabb3d[0, 0] + (0.5 + i) * dedge\n",
    "            position[voxel_id, :] = [x, y, z]\n",
    "            voxel_id += 1\n",
    "dst = h5w.create_dataset(f\"{trg}/position\", compression=\"gzip\", compression_opts=1, data=position)\n",
    "dst.attrs[\"unit\"] = \"nm\"\n",
    "del position\n",
    "\n",
    "voxel_id = identifier_offset\n",
    "coordinate = np.zeros([c, 3], np.uint32)\n",
    "for k in np.arange(0, extent[2]):\n",
    "    for j in np.arange(0, extent[1]):\n",
    "        for i in np.arange(0, extent[0]):\n",
    "            coordinate[voxel_id, :] = [i, j, k]\n",
    "            voxel_id += 1\n",
    "dst = h5w.create_dataset(f\"{trg}/coordinate\", compression=\"gzip\", compression_opts=1, data=coordinate)\n",
    "del coordinate\n",
    "h5w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f61b04e1-9b2f-4631-b0ba-b212038d7719",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a sorted lookup table for all ion positions (implicitly specified by their evaporation_id) O(n*log(n)) time-complexity operation wi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f31fe6de-1bbe-455f-8634-7c9a63e6a64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[( 8, 237,  1) ( 8, 421,  2) ( 8, 725,  3) ( 8, 456,  4) ( 8, 555,  5)\n",
      " ( 8, 398,  6) (29, 327,  7) ( 8, 616,  8) ( 8, 788,  9) (29, 731, 10)]\n",
      "[(0, 205,  2123) (0, 206,  9387) (0, 206, 10955) (0, 206, 11922)\n",
      " (0, 206, 23888) (0, 206, 42471) (0, 206, 52373) (0, 207, 13678)\n",
      " (0, 207, 51341) (0, 209, 53240)]\n",
      "[(72, 1551, 2323047) (72, 1557, 2934723) (72, 1562, 3902798)\n",
      " (72, 1592, 3596344) (72, 1642, 3771729) (72, 1691, 4375588)\n",
      " (72, 1749, 3753706) (72, 1755, 4730446) (72, 1788, 4116453)\n",
      " (72, 1819, 4758833)]\n"
     ]
    }
   ],
   "source": [
    "ion_struct = [('iontype', np.uint8), ('voxel_id', np.uint32), ('evap_id', np.uint32)]\n",
    "lu_ityp_voxel_id_evap_id = np.zeros(n_ions, dtype=ion_struct)\n",
    "lu_ityp_voxel_id_evap_id[\"iontype\"] = ityp[:, 0]\n",
    "del ityp\n",
    "lu_ityp_voxel_id_evap_id[\"voxel_id\"] = voxel_identifier\n",
    "# del voxel_identifier\n",
    "lu_ityp_voxel_id_evap_id[\"evap_id\"] = np.asarray(np.linspace(1, n_ions, num=n_ions, endpoint=True), np.uint32)\n",
    "print(lu_ityp_voxel_id_evap_id[0:10])\n",
    "lu_ityp_voxel_id_evap_id = np.sort(lu_ityp_voxel_id_evap_id, kind=\"stable\", order=[\"iontype\", \"voxel_id\", \"evap_id\"])\n",
    "print(lu_ityp_voxel_id_evap_id[0:10])\n",
    "print(lu_ityp_voxel_id_evap_id[-10::])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcbc4f2d-bf0a-47e7-b9e9-29ecc63d49cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## From this lookup we can very easily now compute the composition table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa22d512-1f4d-42d0-ba4e-4375b406578f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cardinality 36864\n",
      "means we have to visit so that many entries in the lookup table 238176\n",
      "but by virtue of construction of the lookup table all the indices will be close in cache\n",
      "offsets (0, 238175)\n",
      "ityp 0, np.sum(ityp_weights) 238176.0\n",
      "ityp 0, np.sum(total_weights) 238176.0\n",
      "offsets (238176, 238285)\n",
      "ityp 1, np.sum(ityp_weights) 110.0\n",
      "ityp 1, np.sum(total_weights) 238286.0\n",
      "offsets (238286, 238570)\n",
      "ityp 2, np.sum(ityp_weights) 285.0\n",
      "ityp 2, np.sum(total_weights) 238571.0\n",
      "offsets (238571, 241497)\n",
      "ityp 3, np.sum(ityp_weights) 2927.0\n",
      "ityp 3, np.sum(total_weights) 241498.0\n",
      "offsets (241498, 243539)\n",
      "ityp 4, np.sum(ityp_weights) 2042.0\n",
      "ityp 4, np.sum(total_weights) 243540.0\n",
      "offsets (243540, 245454)\n",
      "ityp 5, np.sum(ityp_weights) 1915.0\n",
      "ityp 5, np.sum(total_weights) 245455.0\n",
      "offsets (245455, 264817)\n",
      "ityp 6, np.sum(ityp_weights) 19363.0\n",
      "ityp 6, np.sum(total_weights) 264818.0\n",
      "offsets (264818, 266244)\n",
      "ityp 7, np.sum(ityp_weights) 1427.0\n",
      "ityp 7, np.sum(total_weights) 266245.0\n",
      "offsets (266245, 3816575)\n",
      "ityp 8, np.sum(ityp_weights) 3550331.0\n",
      "ityp 8, np.sum(total_weights) 3816576.0\n",
      "offsets (3816576, 3818362)\n",
      "ityp 9, np.sum(ityp_weights) 1787.0\n",
      "ityp 9, np.sum(total_weights) 3818363.0\n",
      "offsets (3818363, 4060875)\n",
      "ityp 10, np.sum(ityp_weights) 242513.0\n",
      "ityp 10, np.sum(total_weights) 4060876.0\n",
      "offsets (4060876, 4143064)\n",
      "ityp 11, np.sum(ityp_weights) 82189.0\n",
      "ityp 11, np.sum(total_weights) 4143065.0\n",
      "offsets (4143065, 4157278)\n",
      "ityp 12, np.sum(ityp_weights) 14214.0\n",
      "ityp 12, np.sum(total_weights) 4157279.0\n",
      "offsets (4157279, 4157479)\n",
      "ityp 13, np.sum(ityp_weights) 201.0\n",
      "ityp 13, np.sum(total_weights) 4157480.0\n",
      "offsets (4157480, 4159050)\n",
      "ityp 14, np.sum(ityp_weights) 1571.0\n",
      "ityp 14, np.sum(total_weights) 4159051.0\n",
      "offsets (4159051, 4159214)\n",
      "ityp 15, np.sum(ityp_weights) 164.0\n",
      "ityp 15, np.sum(total_weights) 4159215.0\n",
      "offsets (4159215, 4162638)\n",
      "ityp 16, np.sum(ityp_weights) 3424.0\n",
      "ityp 16, np.sum(total_weights) 4162639.0\n",
      "offsets (4162639, 4162858)\n",
      "ityp 17, np.sum(ityp_weights) 220.0\n",
      "ityp 17, np.sum(total_weights) 4162859.0\n",
      "offsets (4162859, 4163016)\n",
      "ityp 18, np.sum(ityp_weights) 158.0\n",
      "ityp 18, np.sum(total_weights) 4163017.0\n",
      "offsets (4163017, 4164221)\n",
      "ityp 19, np.sum(ityp_weights) 1205.0\n",
      "ityp 19, np.sum(total_weights) 4164222.0\n",
      "offsets (4164222, 4171860)\n",
      "ityp 20, np.sum(ityp_weights) 7639.0\n",
      "ityp 20, np.sum(total_weights) 4171861.0\n",
      "offsets (4171861, 4172660)\n",
      "ityp 21, np.sum(ityp_weights) 800.0\n",
      "ityp 21, np.sum(total_weights) 4172661.0\n",
      "offsets (4172661, 4173161)\n",
      "ityp 22, np.sum(ityp_weights) 501.0\n",
      "ityp 22, np.sum(total_weights) 4173162.0\n",
      "offsets (4173162, 4173818)\n",
      "ityp 23, np.sum(ityp_weights) 657.0\n",
      "ityp 23, np.sum(total_weights) 4173819.0\n",
      "offsets (4173819, 4175858)\n",
      "ityp 24, np.sum(ityp_weights) 2040.0\n",
      "ityp 24, np.sum(total_weights) 4175859.0\n",
      "offsets (4175859, 4176393)\n",
      "ityp 25, np.sum(ityp_weights) 535.0\n",
      "ityp 25, np.sum(total_weights) 4176394.0\n",
      "offsets (4176394, 4176727)\n",
      "ityp 26, np.sum(ityp_weights) 334.0\n",
      "ityp 26, np.sum(total_weights) 4176728.0\n",
      "offsets (4176728, 4177078)\n",
      "ityp 27, np.sum(ityp_weights) 351.0\n",
      "ityp 27, np.sum(total_weights) 4177079.0\n",
      "offsets (4177079, 4207091)\n",
      "ityp 28, np.sum(ityp_weights) 30013.0\n",
      "ityp 28, np.sum(total_weights) 4207092.0\n",
      "offsets (4207092, 4771163)\n",
      "ityp 29, np.sum(ityp_weights) 564072.0\n",
      "ityp 29, np.sum(total_weights) 4771164.0\n",
      "offsets (4771164, 4834560)\n",
      "ityp 30, np.sum(ityp_weights) 63397.0\n",
      "ityp 30, np.sum(total_weights) 4834561.0\n",
      "offsets (4834561, 4834796)\n",
      "ityp 31, np.sum(ityp_weights) 236.0\n",
      "ityp 31, np.sum(total_weights) 4834797.0\n",
      "offsets (4834797, 4835056)\n",
      "ityp 32, np.sum(ityp_weights) 260.0\n",
      "ityp 32, np.sum(total_weights) 4835057.0\n",
      "offsets (4835057, 4837141)\n",
      "ityp 33, np.sum(ityp_weights) 2085.0\n",
      "ityp 33, np.sum(total_weights) 4837142.0\n",
      "offsets (4837142, 4837337)\n",
      "ityp 34, np.sum(ityp_weights) 196.0\n",
      "ityp 34, np.sum(total_weights) 4837338.0\n",
      "offsets (4837338, 4838551)\n",
      "ityp 35, np.sum(ityp_weights) 1214.0\n",
      "ityp 35, np.sum(total_weights) 4838552.0\n",
      "offsets (4838552, 4842569)\n",
      "ityp 36, np.sum(ityp_weights) 4018.0\n",
      "ityp 36, np.sum(total_weights) 4842570.0\n",
      "offsets (4842570, 4842981)\n",
      "ityp 37, np.sum(ityp_weights) 412.0\n",
      "ityp 37, np.sum(total_weights) 4842982.0\n",
      "offsets (4842982, 4843201)\n",
      "ityp 38, np.sum(ityp_weights) 220.0\n",
      "ityp 38, np.sum(total_weights) 4843202.0\n",
      "offsets (4843202, 4843727)\n",
      "ityp 39, np.sum(ityp_weights) 526.0\n",
      "ityp 39, np.sum(total_weights) 4843728.0\n",
      "offsets (4843728, 4844470)\n",
      "ityp 40, np.sum(ityp_weights) 743.0\n",
      "ityp 40, np.sum(total_weights) 4844471.0\n",
      "offsets (4844471, 4844573)\n",
      "ityp 41, np.sum(ityp_weights) 103.0\n",
      "ityp 41, np.sum(total_weights) 4844574.0\n",
      "offsets (4844574, 4844876)\n",
      "ityp 42, np.sum(ityp_weights) 303.0\n",
      "ityp 42, np.sum(total_weights) 4844877.0\n",
      "offsets (4844877, 4847694)\n",
      "ityp 43, np.sum(ityp_weights) 2818.0\n",
      "ityp 43, np.sum(total_weights) 4847695.0\n",
      "offsets (4847695, 4849862)\n",
      "ityp 44, np.sum(ityp_weights) 2168.0\n",
      "ityp 44, np.sum(total_weights) 4849863.0\n",
      "offsets (4849863, 4850755)\n",
      "ityp 45, np.sum(ityp_weights) 893.0\n",
      "ityp 45, np.sum(total_weights) 4850756.0\n",
      "offsets (4850756, 4851122)\n",
      "ityp 46, np.sum(ityp_weights) 367.0\n",
      "ityp 46, np.sum(total_weights) 4851123.0\n",
      "offsets (4851123, 4852537)\n",
      "ityp 47, np.sum(ityp_weights) 1415.0\n",
      "ityp 47, np.sum(total_weights) 4852538.0\n",
      "offsets (4852538, 4852795)\n",
      "ityp 48, np.sum(ityp_weights) 258.0\n",
      "ityp 48, np.sum(total_weights) 4852796.0\n",
      "offsets (4852796, 4853119)\n",
      "ityp 49, np.sum(ityp_weights) 324.0\n",
      "ityp 49, np.sum(total_weights) 4853120.0\n",
      "offsets (4853120, 4854499)\n",
      "ityp 50, np.sum(ityp_weights) 1380.0\n",
      "ityp 50, np.sum(total_weights) 4854500.0\n",
      "offsets (4854500, 4855578)\n",
      "ityp 51, np.sum(ityp_weights) 1079.0\n",
      "ityp 51, np.sum(total_weights) 4855579.0\n",
      "offsets (4855579, 4856230)\n",
      "ityp 52, np.sum(ityp_weights) 652.0\n",
      "ityp 52, np.sum(total_weights) 4856231.0\n",
      "offsets (4856231, 4857369)\n",
      "ityp 53, np.sum(ityp_weights) 1139.0\n",
      "ityp 53, np.sum(total_weights) 4857370.0\n",
      "offsets (4857370, 4858607)\n",
      "ityp 54, np.sum(ityp_weights) 1238.0\n",
      "ityp 54, np.sum(total_weights) 4858608.0\n",
      "offsets (4858608, 4859346)\n",
      "ityp 55, np.sum(ityp_weights) 739.0\n",
      "ityp 55, np.sum(total_weights) 4859347.0\n",
      "offsets (4859347, 4861203)\n",
      "ityp 56, np.sum(ityp_weights) 1857.0\n",
      "ityp 56, np.sum(total_weights) 4861204.0\n",
      "offsets (4861204, 4861917)\n",
      "ityp 57, np.sum(ityp_weights) 714.0\n",
      "ityp 57, np.sum(total_weights) 4861918.0\n",
      "offsets (4861918, 4862148)\n",
      "ityp 58, np.sum(ityp_weights) 231.0\n",
      "ityp 58, np.sum(total_weights) 4862149.0\n",
      "offsets (4862149, 4862298)\n",
      "ityp 59, np.sum(ityp_weights) 150.0\n",
      "ityp 59, np.sum(total_weights) 4862299.0\n",
      "offsets (4862299, 4864826)\n",
      "ityp 60, np.sum(ityp_weights) 2528.0\n",
      "ityp 60, np.sum(total_weights) 4864827.0\n",
      "offsets (4864827, 4865048)\n",
      "ityp 61, np.sum(ityp_weights) 222.0\n",
      "ityp 61, np.sum(total_weights) 4865049.0\n",
      "offsets (4865049, 4865140)\n",
      "ityp 62, np.sum(ityp_weights) 92.0\n",
      "ityp 62, np.sum(total_weights) 4865141.0\n",
      "offsets (4865141, 4865892)\n",
      "ityp 63, np.sum(ityp_weights) 752.0\n",
      "ityp 63, np.sum(total_weights) 4865893.0\n",
      "offsets (4865893, 4865991)\n",
      "ityp 64, np.sum(ityp_weights) 99.0\n",
      "ityp 64, np.sum(total_weights) 4865992.0\n",
      "offsets (4865992, 4866558)\n",
      "ityp 65, np.sum(ityp_weights) 567.0\n",
      "ityp 65, np.sum(total_weights) 4866559.0\n",
      "offsets (4866559, 4866787)\n",
      "ityp 66, np.sum(ityp_weights) 229.0\n",
      "ityp 66, np.sum(total_weights) 4866788.0\n",
      "offsets (4866788, 4867157)\n",
      "ityp 67, np.sum(ityp_weights) 370.0\n",
      "ityp 67, np.sum(total_weights) 4867158.0\n",
      "offsets (4867158, 4867262)\n",
      "ityp 68, np.sum(ityp_weights) 105.0\n",
      "ityp 68, np.sum(total_weights) 4867263.0\n",
      "offsets (4867263, 4867410)\n",
      "ityp 69, np.sum(ityp_weights) 148.0\n",
      "ityp 69, np.sum(total_weights) 4867411.0\n",
      "offsets (4867411, 4867528)\n",
      "ityp 70, np.sum(ityp_weights) 118.0\n",
      "ityp 70, np.sum(total_weights) 4867529.0\n",
      "offsets (4867529, 4868054)\n",
      "ityp 71, np.sum(ityp_weights) 526.0\n",
      "ityp 71, np.sum(total_weights) 4868055.0\n",
      "offsets (4868055, 4868201)\n",
      "ityp 72, np.sum(ityp_weights) 147.0\n",
      "ityp 72, np.sum(total_weights) 4868202.0\n",
      "cardinality of cg_grid 36864, n_ions 4868202\n"
     ]
    }
   ],
   "source": [
    "h5w = h5py.File(output_file_name, \"a\")\n",
    "trg = \"/entry1/voxelization/cg_grid\"\n",
    "dst = h5w.create_dataset(f\"{trg}/voxel_identifier\", compression=\"gzip\", compression_opts=1, data=voxel_identifier)\n",
    "\n",
    "c = np.prod(extent)\n",
    "print(f\"cardinality {c}\")\n",
    "# now just add weight/counts for a the iontype-specific part of the lookup-table\n",
    "print(f\"means we have to visit so that many entries in the lookup table {np.sum(lu_ityp_voxel_id_evap_id['iontype'] == 0)}\")\n",
    "print(f\"but by virtue of construction of the lookup table all the indices will be close in cache\")\n",
    "total_weights = np.zeros(c, np.float64)\n",
    "for ityp in np.arange(0, n_ion_types):\n",
    "    inds = np.argwhere(lu_ityp_voxel_id_evap_id[\"iontype\"] == ityp)\n",
    "    offsets = (np.min(inds), np.max(inds))\n",
    "    print(f\"offsets {offsets}\")\n",
    "    # these are inclusive [min, max] array indices to use on lu_ityp_voxel_id_evap_id !\n",
    "    \n",
    "# alternatively one could make two loops where in the first an offset lookup table is generated\n",
    "# after this point one can drop the iontype and evap_id columns from the lu_ityp_voxel_id_evap_id lookup table\n",
    "    ityp_weights = np.zeros(c, np.float64)\n",
    "    for offset in np.arange(offsets[0], offsets[1] + 1):\n",
    "        ityp_weights[lu_ityp_voxel_id_evap_id[\"voxel_id\"][offset]] += 1.\n",
    "    print(f\"ityp {ityp}, np.sum(ityp_weights) {np.sum(ityp_weights)}\")\n",
    "    \n",
    "    # atom/molecular ion-type-specific contribution/intensity/count in each voxel/cell\n",
    "    trg = f\"/entry1/voxelization/ion{ityp}\"\n",
    "    grp = h5w.create_group(f\"{trg}\")\n",
    "    grp.attrs[\"NX_class\"] = \"NXion\"\n",
    "    # dst = h5w.create_dataset(f\"{trg}/name\", data=###)\n",
    "    dst = h5w.create_dataset(f\"{trg}/weight\", compression=\"gzip\", compression_opts=1, data=ityp_weights)\n",
    "    dst.attrs[\"unit\"] = \"a.u.\"  \n",
    "    \n",
    "    total_weights += ityp_weights\n",
    "    print(f\"ityp {ityp}, np.sum(total_weights) {np.sum(total_weights)}\")\n",
    "print(f\"cardinality of cg_grid {c}, n_ions {n_ions}\")\n",
    "\n",
    "# total atom/molecular ion contribution/intensity/count in each voxel/cell\n",
    "trg = f\"/entry1/voxelization\"\n",
    "grp = h5w.create_dataset(f\"{trg}/total\", compression=\"gzip\", compression_opts=1, data=total_weights)\n",
    "grp.attrs[\"unit\"] = \"a.u.\"\n",
    "h5w.close()\n",
    "\n",
    "# reload weights and compute to compositions if really needed as compositions\n",
    "# dst = h5w.create_dataset(f\"{trg}/composition\", compression=\"gzip\", compression_opts=1, data=###)\n",
    "# dst.attrs[\"unit\"] = \"a.u.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8a74215-494e-4c6e-8953-b914d3ec4a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a large number of voxels, say a few million and dozens of iontypes storing all\n",
    "# ityp_weights in main memory might not be useful, instead these should be stored in the HDF5 file\n",
    "# inside the loop and ones the loop is completed, i.e. each total weight for each voxel known\n",
    "# we should update the data in the HDF5 file, alternatively one could also just store the\n",
    "# weights instead of the compositions and then compute the composition with a linear in c*ityp time\n",
    "# complex division, there are even more optimizations one could do, but probably using\n",
    "# multithreading would be a good start before dwelling deeper already this code here is\n",
    "# faster than the original one despite the fact that it works on the entire portland wang\n",
    "# dataset with 4.868 mio ions, while the original test dataset includes only 1.75 mio ions\n",
    "# the top part of the dataset also the code is much shorter to read and eventually even\n",
    "# more robust wrt to how ions are binned with the rectangular transfer function\n",
    "# one should say that this particular implementation (like the original) one needs\n",
    "# substantial modification when one considers a delocalization kernel which spreads\n",
    "# the weight of an ion into the neighboring voxels, this is what paraprobe-nanochem does\n",
    "# one can easily imagine though that the results of this voxelization step can both be\n",
    "# fed into the composition clustering step and here is then also the clear connection\n",
    "# where the capabilities for e.g. the APAV open-source Python library end and Alaukik's\n",
    "# ML/AI work really shines, in fact until now all code including the slicing could have\n",
    "# equally been achieved with paraprobe-nanochem.\n",
    "# Also the excessive reimplementation of file format I/O functions in datautils should\n",
    "# be removed. There is an own Python library for just doing that more robustly and\n",
    "# capable of handling all sorts of molecular ions and charge state analyses included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0546df9b-c182-4082-84ae-a3e2e8390a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/x-hdf5": "/home/mkuehbach/Sprint15/NfdiMatWerkConference/CompositionSpace/tests/apm.composition.space.nxs",
      "text/plain": [
       "<jupyterlab_h5web.widget.H5Web object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H5Web(\"apm.composition.space.nxs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe54195-9309-46a1-8f03-079c61274c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eea000c-7490-4dd5-80bc-5ed87579e739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adb2c85-9a73-40a5-9b97-3ccacfce5732",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.get_big_slices()\n",
    "print(data.chunk_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b030d042-a7f7-435e-8fe1-b64355437ef9",
   "metadata": {},
   "source": [
    "I am convinced that chunking in principle is useful to not have all the data in memory, but this implementation does not care about this.<br>\n",
    "Honestly, I would remove this idea of chunking, all what is happening here is apply an iontype label and split the dataset into chunks<br>\n",
    "the labeling is an O(n) operation, chunking is O(1). Also way to much file overhead and no distinction between charge states...<br>\n",
    "I would recommend starting with the output from a paraprobe-transcoder, paraprobe-ranger run which already uses NeXus, recovers<br>\n",
    "charge states and distinguishes them directly.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea600d03-2996-4bed-b863-0e71fb6a8335",
   "metadata": {},
   "source": [
    "Action points:\n",
    "* Run paraprobe-transcoder on the recon/range file\n",
    "* Run paraprobe-ranger for the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf07f4d1-fa48-4a28-9d08-f5b4576880a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.get_voxels()\n",
    "print(data.voxel_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd7fce5-2fdc-4d95-b054-210232b2a368",
   "metadata": {},
   "source": [
    "The voxelization results in a data structure that is effectively abusing the HDF5 library because unnecessarily groups are created<br>\n",
    "Maybe for the purpose to get quickly which ions are in which voxel but honestly masked operations on numpy arrays<br>\n",
    "would allow one to filter this in a fraction of the time. Also no delocalization is computed.<br>\n",
    "A simple call to paraprobe-nanochem using the rectangular transfer function as a delocalization kernel would yield you the same result.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2542c6-2a41-4e82-9950-1a38c1096fc9",
   "metadata": {},
   "source": [
    "Action points:\n",
    "* Refactor the voxelization to yield one array of the voxel id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c72f69b-16a9-4393-9a32-a1c369da21dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.calculate_voxel_composition()\n",
    "print(data.voxel_ratio_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcf7fe5-4559-4a03-9663-1f5fbab642fc",
   "metadata": {},
   "source": [
    "But there is a key difference to paraprobe: Namely, the computation of the composition for each iontype in each voxel<br>\n",
    "while in paraprobe one has to a priori select for which iontype combination to compute the voxel composition<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016465aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.chunk_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c755d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.voxel_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae07585",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.voxel_ratio_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fda2add",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comps = CompositionClustering(\"experiment_params.yaml\")\n",
    "res = comps.get_PCA_cumsum(data.voxel_ratio_file, data.voxel_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39d8548-592c-418d-a186-0ad5e7fe8962",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(data.voxel_files[0],\"r\") as hdf:\n",
    "    group = hdf.get(\"Group_sm_vox_xyz_Da_spec\")\n",
    "    group0 = hdf.get(\"0\")\n",
    "    spec_lst = list(list(group0.attrs.values())[1])\n",
    "    print(f\"value {spec_lst}, type {type(spec_lst)}, len {len(spec_lst)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1967bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = comps.get_bics_minimization(data.voxel_ratio_file, data.voxel_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9431a6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comps.get_composition_clusters(data.voxel_ratio_file, data.voxel_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9e2f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "comps.generate_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89645fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdata = DataPostprocess(\"experiment_params.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7d8434",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdata.DBSCAN_clustering(comps.voxel_centroid_output_file, cluster_id = 0,\n",
    "                        plot=True, plot3d=True, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ec380f-1b05-4a2e-a210-561eb26b24f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
